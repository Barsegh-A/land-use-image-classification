{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../data/image_labels.csv')\n",
    "one_hot_df = pd.get_dummies(df.drop('id',axis=1), columns=['label1', 'label2', 'label3', 'label4'])\n",
    "one_hot_df.columns.shape\n",
    "one_hot_df.insert(loc=0,column='id',value=df['id'])\n",
    "one_hot_df\n",
    "one_hot_df.to_csv('../data/One_Hot_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 420\n",
      "Validation dataset size: 105\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('../data/One_Hot_labels.csv')\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the sizes of the train and validation sets\n",
    "print(\"Train dataset size:\", len(train_data))\n",
    "print(\"Validation dataset size:\", len(val_data))\n",
    "train_data.to_csv('../data/train.csv')\n",
    "val_data.to_csv('../data/val.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "dataset_path = '../data/land_use/'\n",
    "train_path='../data/land_use/train'\n",
    "val_path='../data/land_use/val'\n",
    "for _, row_train in train_data.iterrows():\n",
    "    image_id = row_train['id']\n",
    "    image_path = os.path.join(dataset_path, image_id + '.jpg')\n",
    "    shutil.copy(image_path, train_path)\n",
    "for _, row_val in val_data.iterrows():\n",
    "    image_id = row_val['id']\n",
    "    image_path = os.path.join(dataset_path, image_id + '.jpg')\n",
    "    shutil.copy(image_path, val_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "class LandUseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_labels, images_path, transform=None):\n",
    "        self.image_labels = image_labels\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, labels = self.image_labels[idx]\n",
    "        # Load the JPG image containing 4 sub-pictures\n",
    "        image_filename = f'{image_id}.jpg'\n",
    "        image_filepath = f'{self.images_path}/{image_filename}'\n",
    "        image = PIL.Image.open(image_filepath)\n",
    "        # Preprocess the image if a transform is provided\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        # Convert labels to tensors\n",
    "        labels = torch.nn.functional.one_hot(torch.tensor(self.image_labels),21)\n",
    "        return image,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        \n",
    "        # Define your model architecture layers here\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, 21)\n",
    "        self.num_classes=num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def backpropagation(self, optimizer, criterion, inputs, labels):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = self.forward(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the model parameters\n",
    "    \n",
    "    def optimize(self, train_loader, num_epochs, learning_rate):\n",
    "        criterion = nn.CrossEntropyLoss()  # Define the loss criterion\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)  # Define the optimizer\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                self.backpropagation(optimizer, criterion, inputs, labels)\n",
    "                running_loss += criterion(self.forward(inputs), labels).item()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TrainEval:\n",
    "    def __init__(self, epochs, model, train_dataloader, val_dataloader, optimizer, criterion, device, model_name):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        \n",
    "\n",
    "    def train_fn(self, current_epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        tk = tqdm(self.train_dataloader, desc=f\"EPOCH [{current_epoch+1}/{self.epochs}] [TRAIN]\")\n",
    "\n",
    "        for t, data in enumerate(tk):\n",
    "            images, labels = data\n",
    "\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(images)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            tk.set_postfix({\"Loss\": \"{:.6f}\".format(total_loss / (t + 1))})\n",
    "\n",
    "        return total_loss / len(self.train_dataloader)\n",
    "\n",
    "    def eval_fn(self, current_epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        tk = tqdm(self.val_dataloader, desc=f\"EPOCH [{current_epoch+1}/{self.epochs}] [VALID]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t, data in enumerate(tk):\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                logits = self.model(images)\n",
    "                loss = self.criterion(logits, labels)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                tk.set_postfix({\"Loss\": \"{:.6f}\".format(total_loss / (t + 1))})\n",
    "\n",
    "        return total_loss / len(self.val_dataloader)\n",
    "\n",
    "    def train(self):\n",
    "        best_valid_loss = float('inf')\n",
    "        best_train_loss = float('inf')\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self.train_fn(epoch)\n",
    "            val_loss = self.eval_fn(epoch)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss < best_valid_loss:\n",
    "                torch.save(self.model.state_dict(), f\"{self.model_name}_best_weights.pt\")\n",
    "                print(\"Saved Best Weights\")\n",
    "                best_valid_loss = val_loss\n",
    "                best_train_loss = train_loss\n",
    "\n",
    "        print(f\"Training Loss: {best_train_loss}\")\n",
    "        print(f\"Validation Loss: {best_valid_loss}\")\n",
    "\n",
    "        self.train_losses = train_losses\n",
    "        self.val_losses = val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "num_classes = 21\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "image_dir = '../data/land_use'\n",
    "csv_file = '../data/train.csv'\n",
    "\n",
    "# Define transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = LandUseDataset(csv_file, image_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ImageClassifier(num_classes)\n",
    "\n",
    "# Define the loss criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in ./data/land_use/train/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 26\u001b[0m\n\u001b[0;32m     19\u001b[0m transform \u001b[39m=\u001b[39m Compose([\n\u001b[0;32m     20\u001b[0m     Resize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),\n\u001b[0;32m     21\u001b[0m     ToTensor(),\n\u001b[0;32m     22\u001b[0m     Normalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])  \u001b[39m# Normalize with ImageNet mean and std\u001b[39;00m\n\u001b[0;32m     23\u001b[0m ])\n\u001b[0;32m     25\u001b[0m \u001b[39m# Load the dataset and split into train and validation sets\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m dataset \u001b[39m=\u001b[39m ImageFolder(dataset_path, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Load the CSV file\u001b[39;00m\n\u001b[0;32m     29\u001b[0m train_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_csv(train_csv_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[0;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m     \u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\datasets\\folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     40\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[0;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in ./data/land_use/train/."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = './data/land_use/train/'\n",
    "# Path to the CSV file\n",
    "train_csv_path = '../data/train.csv'\n",
    "val_csv_path=\"../data/val.csv\"\n",
    "\n",
    "# Define the transforms for preprocessing the images\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "# Load the dataset and split into train and validation sets\n",
    "dataset = LandUseDataset(dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# Load the CSV file\n",
    "train_df=pd.read_csv(train_csv_path)\n",
    "val_df=pd.read_csv(val_csv_path)\n",
    "# Split the dataset into train and validation sets\n",
    "# Get the IDs for the train and validation images\n",
    "train_ids = train_df['id'].tolist()\n",
    "val_ids = val_df['id'].tolist()\n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_dataset = torch.utils.data.Subset(dataset, [dataset.samples.index(sample) for sample in dataset.samples if sample[0].split('/')[-1].split('.')[0] in train_ids])\n",
    "val_dataset = torch.utils.data.Subset(dataset, [dataset.samples.index(sample) for sample in dataset.samples if sample[0].split('/')[-1].split('.')[0] in val_ids])\n",
    "\n",
    "# Define the dataloaders\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define the model\n",
    "model = resnet50(pretrained=True)\n",
    "num_classes = 21\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the training and evaluation loop\n",
    "def train_eval(epochs, model, train_dataloader, val_dataloader, optimizer, criterion, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print training and validation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
